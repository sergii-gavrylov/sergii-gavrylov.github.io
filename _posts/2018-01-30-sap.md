---
layout: post
title: "Emergence of Language with Multi-agent Games"
date: 2018-02-15 10:27:21 +0100
author: "Serhii Havrylov"
permalink: /blog/sap/
comments: true
mathjax: true
hidden: true
---

# Intro
Communication is by far one of the most impressive human abilities. Human civilisation was able to accumulate an enormous amount of knowledge and pass it to the next generations just because we can understand and use language. Origin of language is a mystery that has captivated people's minds for centuries. The more general question of how communication arises has been studied for a long time as well. Yet, due to the algorithmic and computational limitations, just before recent, almost all mathematical models of this miraculous process had to be restricted to a low dimensional simple observation spaces. However, in the past years, there was a considerable amount of interest to this problem from the deep learning community. Hence, in this post, we would like to present our contribution to the field.


# Referential game
One of the most basic challenges of using a language is to refer to things. Thus, it is not surprising that a [referential game](http://onlinelibrary.wiley.com/book/10.1002/9780470693711) is a go-to setting in learning-to-communicate field. Many extensions to the primary referential game are possible. In our case we decided to proceed with the following setup:

<a name="rg"></a>![image-rg](/res/rg.png){: .align-image-center}

<!-- 1. There is a collection of images \\(\\{i_n\\}\_{n=1}^N\\) from which a target image \\(t\\) is sampled as well as \\(K\\) distracting images \\(\\{d_k\\}\_{k=1}^K\\).
2. There are two agents: a sender \\(S\_{\phi}\\) and a receiver \\(R\_{\theta}\\).
3. After seeing the target image \\( t \\), the sender has to come up with a message \\(m_t\\), which is represented by a sequence of symbols from the vocabulary \\( V \\) of a size \\( \|V\| \\). The maximum possible length of a sequence is \\( L \\).
4. Given the message \\(m\_t\\) and a set of images, which consists of distracting images and the target image, the goal of the receiver is to identify the target image correctly. -->

1. There is a collection of images from which a target image is sampled as well as \\(K\\) distracting images.
2. There are two agents: a sender and a receiver.
3. After seeing the target image, the sender has to come up with a message that is represented by a sequence of symbols from the vocabulary of a fixed size. There is the maximum possible length of a sequence.
4. Given the generated message and a set of images that consists of distracting images and the target image, the goal of the receiver is to identify the target image correctly.

Therefore, to succeed in this referential game a sender has to choose the words carefully and put them in a sequence in such a way that will make it easy for a receiver to correctly identify what image was shown to a sender. Compared to the [previous](http://iopscience.iop.org/article/10.1088/1742-5468/2006/06/P06014) [work](https://arxiv.org/abs/1612.07182) there is an essential difference: for example, we use sequences rather than single symbols to generate messages. This makes our setting arguably more realistic and challenging from the learning perspective.


# Agents
Both agents, a sender and a receiver, are implemented as [recurrent](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) neural networks, namely [long](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735) short-term memory networks. This is one of the standard tools for generating and processing sequences. The figure below shows the sketch of a model. Solid arrows represent deterministic computations. Dashed arrows depict copying previously-obtained word. And lastly, diamond-shaped arrows mean sampling word from the vocabulary.

![image-sr](/res/SR.gif){: .align-image-center}

Probably this is the most important and the most troublesome part of the model. It is a crucial element because this is the place where a sender makes decisions about what to say next. On the other hand, it is troublesome because it is stochastic. Unfortunately, ubiquitous backpropagation algorithm relies on having chains of continuous differentiable functions in each of the layers of the neural network. However, this particular architecture contains nondifferentiable sampling from the discrete probability distribution. So, we can't use backpropagation right away. The visual system of a sender is implemented as the convolutional neural network (CNN). In our case, it had already been pretrained neural network with [VGG-16](https://arxiv.org/abs/1409.1556) architecture. So, eventually, images are represented by outputs of the penultimate hidden layer of the CNN. It is a low dimensional dense vector that summarises semantic information about a particular image. As you can see from the figure above, a message is obtained by sequentially sampling until the maximum possible length is reached or the special token "_end_ _of_ _a_ _message_" is generated.

## Learning
It is relatively easy to learn a receiver agent. It is end-to-end differentiable so gradients of the loss function with respect to its parameters can be estimated efficiently. The real challenge is to learn the sender agent. Its computational graph contains sampling, which makes it nondifferentiable. As a baseline, we implemented [REINFORCE](https://link.springer.com/chapter/10.1007/978-1-4615-3618-5_2) algorithm. This is a method that provides a simple way of estimating gradients of the loss function with respect to parameters of the stochastic policy. Even though it is unbiased, usually it has a huge variance and this fact slows down learning of a model. Fortunately, last year two groups independently discovered a biased but low-variance estimator - [Gumbel](https://arxiv.org/abs/1611.01144)-Softmax [estimator](https://arxiv.org/abs/1611.00712) (GS estimator). It allows to relax original discrete variable with its continuous counterpart. This makes everything differentiable and so backpropagation algorithm can be applied. This topic is quite big and deserves its own post, so, if you are interested, I encourage you to read a [blog post](https://blog.evjang.com/2016/11/tutorial-categorical-variational.html) from one of the authors of this method.


# Our findings
The first thing we examined after learning the model was communication success rate. Communication between two agents is successful if the target image was identified correctly. As one can see from the figure below, Gumbel-Softmax estimator (red and blue curves) are better than REINFORCE (yellow and green curves) in all cases except when agents are allowed to communicate only by one word.
![image-comac](/res/comac.png){: .align-image-center-smaller}
Probably, in this relatively simple situation, the variance for REINFORCE is not an issue and the property of being unbiased paid off. While bias of GS estimator drifted it away from the optimal solution. Also, this plot does go in hand with intuition and clearly shows that by using more words one can describe an image more precisely. We also investigated how many interactions between agent have to be performed to learn the communication protocol (left image in the figure below). Much to our surprise, we saw that the number of updates that are required to achieve training convergence with the Gumbel-Softmax estimator (green curve) decreases when we let a sender use longer messages. This behaviour is slightly unintuitive as one could expect that it is harder to learn the protocol when the search space of communication protocols is larger. In other words, using longer sequences helps to learn a communication protocol faster. However, this is not the case for the REINFORCE estimator (red curve): it usually takes five-fold more updates to converge compared to GS estimator and also there is no clear dependency between the number of updates needed to converge and the maximum possible length of a message.
<div class="photo-center-images">
	<img src="/res/nupdates.png"/>
	<img src="/res/perp.png"/>
</div>

We also plot the perplexity of the encoder (right image in the figure above), which arguably measures how many options sender has to choose from on each time step while sampling from the probability distribution over vocabulary. It is relatively high and increasing with sentence length for GS estimator (green curve), whereas for REINFORCE (red curve) the perplexity increase is not as rapid. This implies redundancy in the encodings: there exist multiple paraphrases that encode the same semantic content.

So, how does the learned language look like? To better understand the nature of it, we inspected a small subset of sentences
that were produced by the model with maximum possible message length equal to 5.  First, we took a random photo of an object and generated a message. Then we iterated over the dataset and randomly selected images with messages that share prefixes of 1, 2 and 3 symbols with the generated message. For example, the first row of a left image shows some samples that correspond to **(5747 * * * *)** code. Here "\*" means any word from the vocabulary or end-of-sentence padding. Images in this subset depict animals.
![image-comac](/res/prot.png){: .align-image-center}

On the other hand, it seems that images for **(* * * 5747 *)** code do not correspond to any predefined category. This suggests that word order is crucial in the developed language. Particularly, word 5747 on the first position encodes presence of an animal in the image. The same figure shows that message **(5747 5747 7125 * *)** corresponds to a particular species of bears. This suggests that the developed language implements some kind of hierarchical coding, yet the model was not constrained explicitly to use any hierarchical encoding scheme. Presumably, this can help the model efficiently describe unseen images. Nevertheless, natural language uses other principles to ensure compositionality. The model shows similar behaviour for images in the food domain (right image in the figure above) as well.



# Conclusion
In our work, we have shown that agents, modelled using neural networks, can successfully invent a language that consists of sequences of discrete tokens. We also found that agents develop communication protocol faster when we allow them to use longer sequences of symbols. We observed that the induced language implements hierarchical encoding scheme and there exist multiple paraphrases that encode the same semantic content. In the future work, we would like to extend this approach to modelling practical dialogues. The _"game"_ can be played between two agents rather than an agent and a human while human
interpretability would be ensured by integrating supervised loss into the learning objective. Hopefully, this will reduce the amount of necessary human supervision. You can find more information and the technical details of our research in this paper: [Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols](https://papers.nips.cc/paper/6810-emergence-of-language-with-multi-agent-games-learning-to-communicate-with-sequences-of-symbols).
